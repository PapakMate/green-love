{
  "metadata": {
    "description": "Crusoe Cloud GPU instance pricing",
    "source": "https://www.crusoe.ai/cloud/pricing#gpu-instances",
    "last_updated": "2025-12",
    "price_unit": "USD per GPU-hour"
  },
  "gpus": {
    "H200 HGX 141GB": {
      "vram_gb": 141,
      "on_demand_per_gpu_hr": 4.29,
      "spot_per_gpu_hr": null,
      "benchmark_key": "H100 80GB SXM5",
      "scale_factor": 1.3,
      "scale_note": "H200 estimated at ~1.3x H100 SXM5 based on published specs (HBM3e bandwidth + same SM count)"
    },
    "H100 HGX 80GB": {
      "vram_gb": 80,
      "on_demand_per_gpu_hr": 3.90,
      "spot_per_gpu_hr": 1.60,
      "benchmark_key": "H100 80GB SXM5",
      "scale_factor": 1.0,
      "scale_note": "Direct match to Lambda Labs H100 80GB SXM5 benchmark"
    },
    "A100 SXM 80GB": {
      "vram_gb": 80,
      "on_demand_per_gpu_hr": 1.95,
      "spot_per_gpu_hr": 1.30,
      "benchmark_key": "A100 80GB SXM4",
      "scale_factor": 1.0,
      "scale_note": "Direct match to Lambda Labs A100 80GB SXM4 benchmark"
    },
    "A100 PCIe 80GB": {
      "vram_gb": 80,
      "on_demand_per_gpu_hr": 1.65,
      "spot_per_gpu_hr": 1.20,
      "benchmark_key": "A100 80GB PCIe",
      "scale_factor": 1.0,
      "scale_note": "Direct match to Lambda Labs A100 80GB PCIe benchmark"
    },
    "A100 PCIe 40GB": {
      "vram_gb": 40,
      "on_demand_per_gpu_hr": 1.45,
      "spot_per_gpu_hr": 1.00,
      "benchmark_key": "A100 40GB PCIe",
      "scale_factor": 1.0,
      "scale_note": "Direct match to Lambda Labs A100 40GB PCIe benchmark"
    },
    "L40S 48GB": {
      "vram_gb": 48,
      "on_demand_per_gpu_hr": 1.00,
      "spot_per_gpu_hr": 0.50,
      "benchmark_key": "RTX 6000 Ada",
      "scale_factor": 0.85,
      "scale_note": "L40S uses same Ada Lovelace arch as RTX 6000 Ada but fewer SMs; estimated ~0.85x throughput"
    },
    "A40 48GB": {
      "vram_gb": 48,
      "on_demand_per_gpu_hr": 0.90,
      "spot_per_gpu_hr": 0.40,
      "benchmark_key": "RTX A6000",
      "scale_factor": 0.90,
      "scale_note": "A40 uses same Ampere GA102 die as RTX A6000 with similar SM count; estimated ~0.9x throughput"
    },
    "MI300X 192GB": {
      "vram_gb": 192,
      "on_demand_per_gpu_hr": 3.25,
      "spot_per_gpu_hr": 0.95,
      "benchmark_key": "H100 80GB SXM5",
      "scale_factor": 1.0,
      "scale_note": "AMD MI300X estimated ~1.0x H100 SXM5 for PyTorch training based on published MLPerf results"
    }
  }
}
